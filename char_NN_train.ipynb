{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from skimage import io, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_numpy_array(folder_path):\n",
    "    image_arrays = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through each folder (assuming each folder represents a label)\n",
    "    for label in os.listdir(folder_path):\n",
    "        label_path = os.path.join(folder_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        # Read images from each folder\n",
    "        for filename in os.listdir(label_path):\n",
    "            image_path = os.path.join(label_path, filename)\n",
    "            if os.path.isfile(image_path) and filename.endswith(('.png')):\n",
    "                # Open image using scikit-image\n",
    "                image = io.imread(image_path, as_gray=True)\n",
    "\n",
    "                # Convert image to numpy array\n",
    "                image_array = np.array(image)\n",
    "\n",
    "                # Append to list\n",
    "                image_arrays.append(image_array)\n",
    "                labels.append(label.replace('_', ''))\n",
    "                # print(f'Image label: {label.replace(\"_\", \"\")}, image_file: {image_path}')\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    image_arrays = np.array(image_arrays)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return image_arrays, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "images, labels = images_to_numpy_array('dataset')\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import transforms\n",
    "\n",
    "from myutils import *\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_FOLDER = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "folder_path = \"dataset\"\n",
    "images, eng_labels = images_to_numpy_array(folder_path)\n",
    "\n",
    "# Normalize images and encode labels\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(eng_labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a dictionary of labels and their inverse transformed values\n",
    "label_dict = [label_encoder.inverse_transform([label])[0] for label in set(labels)]\n",
    "\n",
    "# Write the dictionary to a file\n",
    "with open('labels.list', 'w') as file:\n",
    "    json.dump(label_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for data augmentation (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(X_train, y_train, transform=transform)\n",
    "test_dataset = CustomDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "# Define dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created network for labelling 18 classes\n",
      "Epoch [1/600], Loss: 4.2913\n",
      "Epoch [2/600], Loss: 4.2200\n",
      "Epoch [3/600], Loss: 4.1953\n",
      "Epoch [4/600], Loss: 4.1576\n",
      "Epoch [5/600], Loss: 4.0917\n",
      "Epoch [6/600], Loss: 4.0076\n",
      "Epoch [7/600], Loss: 3.9074\n",
      "Epoch [8/600], Loss: 3.7937\n",
      "Epoch [9/600], Loss: 3.6746\n",
      "Epoch [10/600], Loss: 3.5554\n",
      "Epoch [11/600], Loss: 3.4314\n",
      "Epoch [12/600], Loss: 3.3247\n",
      "Epoch [13/600], Loss: 3.2120\n",
      "Epoch [14/600], Loss: 3.1129\n",
      "Epoch [15/600], Loss: 3.0352\n",
      "Epoch [16/600], Loss: 2.9325\n",
      "Epoch [17/600], Loss: 2.8585\n",
      "Epoch [18/600], Loss: 2.8019\n",
      "Epoch [19/600], Loss: 2.7276\n",
      "Epoch [20/600], Loss: 2.6482\n",
      "Epoch [21/600], Loss: 2.5948\n",
      "Epoch [22/600], Loss: 2.5412\n",
      "Epoch [23/600], Loss: 2.4965\n",
      "Epoch [24/600], Loss: 2.4370\n",
      "Epoch [25/600], Loss: 2.3647\n",
      "Epoch [26/600], Loss: 2.3359\n",
      "Epoch [27/600], Loss: 2.2804\n",
      "Epoch [28/600], Loss: 2.2443\n",
      "Epoch [29/600], Loss: 2.1984\n",
      "Epoch [30/600], Loss: 2.1617\n",
      "Epoch [31/600], Loss: 2.1144\n",
      "Epoch [32/600], Loss: 2.0817\n",
      "Epoch [33/600], Loss: 2.0512\n",
      "Epoch [34/600], Loss: 2.0144\n",
      "Epoch [35/600], Loss: 1.9777\n",
      "Epoch [36/600], Loss: 1.9570\n",
      "Epoch [37/600], Loss: 1.9308\n",
      "Epoch [38/600], Loss: 1.9102\n",
      "Epoch [39/600], Loss: 1.8884\n",
      "Epoch [40/600], Loss: 1.8518\n",
      "Epoch [41/600], Loss: 1.8272\n",
      "Epoch [42/600], Loss: 1.8041\n",
      "Epoch [43/600], Loss: 1.7895\n",
      "Epoch [44/600], Loss: 1.7543\n",
      "Epoch [45/600], Loss: 1.7298\n",
      "Epoch [46/600], Loss: 1.7208\n",
      "Epoch [47/600], Loss: 1.7038\n",
      "Epoch [48/600], Loss: 1.6791\n",
      "Epoch [49/600], Loss: 1.6711\n",
      "Epoch [50/600], Loss: 1.6472\n",
      "Epoch [51/600], Loss: 1.6284\n",
      "Epoch [52/600], Loss: 1.6397\n",
      "Epoch [53/600], Loss: 1.5983\n",
      "Epoch [54/600], Loss: 1.5839\n",
      "Epoch [55/600], Loss: 1.5642\n",
      "Epoch [56/600], Loss: 1.5509\n",
      "Epoch [57/600], Loss: 1.5393\n",
      "Epoch [58/600], Loss: 1.5220\n",
      "Epoch [59/600], Loss: 1.5028\n",
      "Epoch [60/600], Loss: 1.5006\n",
      "Epoch [61/600], Loss: 1.4814\n",
      "Epoch [62/600], Loss: 1.4833\n",
      "Epoch [63/600], Loss: 1.4616\n",
      "Epoch [64/600], Loss: 1.4481\n",
      "Epoch [65/600], Loss: 1.4487\n",
      "Epoch [66/600], Loss: 1.4294\n",
      "Epoch [67/600], Loss: 1.4290\n",
      "Epoch [68/600], Loss: 1.4005\n",
      "Epoch [69/600], Loss: 1.3989\n",
      "Epoch [70/600], Loss: 1.3918\n",
      "Epoch [71/600], Loss: 1.3853\n",
      "Epoch [72/600], Loss: 1.3683\n",
      "Epoch [73/600], Loss: 1.3600\n",
      "Epoch [74/600], Loss: 1.3498\n",
      "Epoch [75/600], Loss: 1.3371\n",
      "Epoch [76/600], Loss: 1.3295\n",
      "Epoch [77/600], Loss: 1.3282\n",
      "Epoch [78/600], Loss: 1.3069\n",
      "Epoch [79/600], Loss: 1.3011\n",
      "Epoch [80/600], Loss: 1.2901\n",
      "Epoch [81/600], Loss: 1.2833\n",
      "Epoch [82/600], Loss: 1.2716\n",
      "Epoch [83/600], Loss: 1.2814\n",
      "Epoch [84/600], Loss: 1.2738\n",
      "Epoch [85/600], Loss: 1.2526\n",
      "Epoch [86/600], Loss: 1.2660\n",
      "Epoch [87/600], Loss: 1.2484\n",
      "Epoch [88/600], Loss: 1.2433\n",
      "Epoch [89/600], Loss: 1.2325\n",
      "Epoch [90/600], Loss: 1.2160\n",
      "Epoch [91/600], Loss: 1.2176\n",
      "Epoch [92/600], Loss: 1.1983\n",
      "Epoch [93/600], Loss: 1.1859\n",
      "Epoch [94/600], Loss: 1.1864\n",
      "Epoch [95/600], Loss: 1.1897\n",
      "Epoch [96/600], Loss: 1.1852\n",
      "Epoch [97/600], Loss: 1.1711\n",
      "Epoch [98/600], Loss: 1.1626\n",
      "Epoch [99/600], Loss: 1.1526\n",
      "Epoch [100/600], Loss: 1.1490\n",
      "Epoch [101/600], Loss: 1.1490\n",
      "Epoch [102/600], Loss: 1.1464\n",
      "Epoch [103/600], Loss: 1.1305\n",
      "Epoch [104/600], Loss: 1.1304\n",
      "Epoch [105/600], Loss: 1.1241\n",
      "Epoch [106/600], Loss: 1.1134\n",
      "Epoch [107/600], Loss: 1.1244\n",
      "Epoch [108/600], Loss: 1.1218\n",
      "Epoch [109/600], Loss: 1.1032\n",
      "Epoch [110/600], Loss: 1.0879\n",
      "Epoch [111/600], Loss: 1.0928\n",
      "Epoch [112/600], Loss: 1.0802\n",
      "Epoch [113/600], Loss: 1.0696\n",
      "Epoch [114/600], Loss: 1.0733\n",
      "Epoch [115/600], Loss: 1.0680\n",
      "Epoch [116/600], Loss: 1.0730\n",
      "Epoch [117/600], Loss: 1.0619\n",
      "Epoch [118/600], Loss: 1.0514\n",
      "Epoch [119/600], Loss: 1.0529\n",
      "Epoch [120/600], Loss: 1.0444\n",
      "Epoch [121/600], Loss: 1.0365\n",
      "Epoch [122/600], Loss: 1.0255\n",
      "Epoch [123/600], Loss: 1.0242\n",
      "Epoch [124/600], Loss: 1.0232\n",
      "Epoch [125/600], Loss: 1.0201\n",
      "Epoch [126/600], Loss: 1.0242\n",
      "Epoch [127/600], Loss: 1.0044\n",
      "Epoch [128/600], Loss: 0.9954\n",
      "Epoch [129/600], Loss: 0.9997\n",
      "Epoch [130/600], Loss: 0.9857\n",
      "Epoch [131/600], Loss: 0.9903\n",
      "Epoch [132/600], Loss: 0.9909\n",
      "Epoch [133/600], Loss: 0.9792\n",
      "Epoch [134/600], Loss: 0.9670\n",
      "Epoch [135/600], Loss: 0.9749\n",
      "Epoch [136/600], Loss: 0.9820\n",
      "Epoch [137/600], Loss: 0.9763\n",
      "Epoch [138/600], Loss: 0.9583\n",
      "Epoch [139/600], Loss: 0.9469\n",
      "Epoch [140/600], Loss: 0.9627\n",
      "Epoch [141/600], Loss: 0.9396\n",
      "Epoch [142/600], Loss: 0.9414\n",
      "Epoch [143/600], Loss: 0.9409\n",
      "Epoch [144/600], Loss: 0.9220\n",
      "Epoch [145/600], Loss: 0.9254\n",
      "Epoch [146/600], Loss: 0.9343\n",
      "Epoch [147/600], Loss: 0.9204\n",
      "Epoch [148/600], Loss: 0.9215\n",
      "Epoch [149/600], Loss: 0.9164\n",
      "Epoch [150/600], Loss: 0.9109\n",
      "Epoch [151/600], Loss: 0.9019\n",
      "Epoch [152/600], Loss: 0.8862\n",
      "Epoch [153/600], Loss: 0.9004\n",
      "Epoch [154/600], Loss: 0.8971\n",
      "Epoch [155/600], Loss: 0.8856\n",
      "Epoch [156/600], Loss: 0.8935\n",
      "Epoch [157/600], Loss: 0.8906\n",
      "Epoch [158/600], Loss: 0.8763\n",
      "Epoch [159/600], Loss: 0.8662\n",
      "Epoch [160/600], Loss: 0.8830\n",
      "Epoch [161/600], Loss: 0.8529\n",
      "Epoch [162/600], Loss: 0.8575\n",
      "Epoch [163/600], Loss: 0.8706\n",
      "Epoch [164/600], Loss: 0.8688\n",
      "Epoch [165/600], Loss: 0.8541\n",
      "Epoch [166/600], Loss: 0.8433\n",
      "Epoch [167/600], Loss: 0.8401\n",
      "Epoch [168/600], Loss: 0.8553\n",
      "Epoch [169/600], Loss: 0.8425\n",
      "Epoch [170/600], Loss: 0.8317\n",
      "Epoch [171/600], Loss: 0.8515\n",
      "Epoch [172/600], Loss: 0.8348\n",
      "Epoch [173/600], Loss: 0.8418\n",
      "Epoch [174/600], Loss: 0.8224\n",
      "Epoch [175/600], Loss: 0.8198\n",
      "Epoch [176/600], Loss: 0.8145\n",
      "Epoch [177/600], Loss: 0.8032\n",
      "Epoch [178/600], Loss: 0.8090\n",
      "Epoch [179/600], Loss: 0.8139\n",
      "Epoch [180/600], Loss: 0.8027\n",
      "Epoch [181/600], Loss: 0.8083\n",
      "Epoch [182/600], Loss: 0.8018\n",
      "Epoch [183/600], Loss: 0.7912\n",
      "Epoch [184/600], Loss: 0.7996\n",
      "Epoch [185/600], Loss: 0.7824\n",
      "Epoch [186/600], Loss: 0.7796\n",
      "Epoch [187/600], Loss: 0.7730\n",
      "Epoch [188/600], Loss: 0.7662\n",
      "Epoch [189/600], Loss: 0.7713\n",
      "Epoch [190/600], Loss: 0.7713\n",
      "Epoch [191/600], Loss: 0.7568\n",
      "Epoch [192/600], Loss: 0.7659\n",
      "Epoch [193/600], Loss: 0.7727\n",
      "Epoch [194/600], Loss: 0.7583\n",
      "Epoch [195/600], Loss: 0.7574\n",
      "Epoch [196/600], Loss: 0.7500\n",
      "Epoch [197/600], Loss: 0.7405\n",
      "Epoch [198/600], Loss: 0.7521\n",
      "Epoch [199/600], Loss: 0.7422\n",
      "Epoch [200/600], Loss: 0.7374\n",
      "Epoch [201/600], Loss: 0.7345\n",
      "Epoch [202/600], Loss: 0.7387\n",
      "Epoch [203/600], Loss: 0.7333\n",
      "Epoch [204/600], Loss: 0.7353\n",
      "Epoch [205/600], Loss: 0.7204\n",
      "Epoch [206/600], Loss: 0.7280\n",
      "Epoch [207/600], Loss: 0.7143\n",
      "Epoch [208/600], Loss: 0.7275\n",
      "Epoch [209/600], Loss: 0.7070\n",
      "Epoch [210/600], Loss: 0.7260\n",
      "Epoch [211/600], Loss: 0.7238\n",
      "Epoch [212/600], Loss: 0.7162\n",
      "Epoch [213/600], Loss: 0.7088\n",
      "Epoch [214/600], Loss: 0.6957\n",
      "Epoch [215/600], Loss: 0.7041\n",
      "Epoch [216/600], Loss: 0.7060\n",
      "Epoch [217/600], Loss: 0.6861\n",
      "Epoch [218/600], Loss: 0.6882\n",
      "Epoch [219/600], Loss: 0.6925\n",
      "Epoch [220/600], Loss: 0.6950\n",
      "Epoch [221/600], Loss: 0.6873\n",
      "Epoch [222/600], Loss: 0.6762\n",
      "Epoch [223/600], Loss: 0.6654\n",
      "Epoch [224/600], Loss: 0.6738\n",
      "Epoch [225/600], Loss: 0.6687\n",
      "Epoch [226/600], Loss: 0.6655\n",
      "Epoch [227/600], Loss: 0.6673\n",
      "Epoch [228/600], Loss: 0.6508\n",
      "Epoch [229/600], Loss: 0.6543\n",
      "Epoch [230/600], Loss: 0.6506\n",
      "Epoch [231/600], Loss: 0.6496\n",
      "Epoch [232/600], Loss: 0.6509\n",
      "Epoch [233/600], Loss: 0.6631\n",
      "Epoch [234/600], Loss: 0.6659\n",
      "Epoch [235/600], Loss: 0.6497\n",
      "Epoch [236/600], Loss: 0.6459\n",
      "Epoch [237/600], Loss: 0.6338\n",
      "Epoch [238/600], Loss: 0.6560\n",
      "Epoch [239/600], Loss: 0.6349\n",
      "Epoch [240/600], Loss: 0.6359\n",
      "Epoch [241/600], Loss: 0.6258\n",
      "Epoch [242/600], Loss: 0.6396\n",
      "Epoch [243/600], Loss: 0.6294\n",
      "Epoch [244/600], Loss: 0.6242\n",
      "Epoch [245/600], Loss: 0.6196\n",
      "Epoch [246/600], Loss: 0.6232\n",
      "Epoch [247/600], Loss: 0.6377\n",
      "Epoch [248/600], Loss: 0.6260\n",
      "Epoch [249/600], Loss: 0.6198\n",
      "Epoch [250/600], Loss: 0.6128\n",
      "Epoch [251/600], Loss: 0.6286\n",
      "Epoch [252/600], Loss: 0.6056\n",
      "Epoch [253/600], Loss: 0.6095\n",
      "Epoch [254/600], Loss: 0.6074\n",
      "Epoch [255/600], Loss: 0.6021\n",
      "Epoch [256/600], Loss: 0.6101\n",
      "Epoch [257/600], Loss: 0.6013\n",
      "Epoch [258/600], Loss: 0.6050\n",
      "Epoch [259/600], Loss: 0.6034\n",
      "Epoch [260/600], Loss: 0.5879\n",
      "Epoch [261/600], Loss: 0.5798\n",
      "Epoch [262/600], Loss: 0.5897\n",
      "Epoch [263/600], Loss: 0.5785\n",
      "Epoch [264/600], Loss: 0.5772\n",
      "Epoch [265/600], Loss: 0.5806\n",
      "Epoch [266/600], Loss: 0.5874\n",
      "Epoch [267/600], Loss: 0.5665\n",
      "Epoch [268/600], Loss: 0.5720\n",
      "Epoch [269/600], Loss: 0.5771\n",
      "Epoch [270/600], Loss: 0.5754\n",
      "Epoch [271/600], Loss: 0.5715\n",
      "Epoch [272/600], Loss: 0.5622\n",
      "Epoch [273/600], Loss: 0.5604\n",
      "Epoch [274/600], Loss: 0.5530\n",
      "Epoch [275/600], Loss: 0.5573\n",
      "Epoch [276/600], Loss: 0.5645\n",
      "Epoch [277/600], Loss: 0.5570\n",
      "Epoch [278/600], Loss: 0.5518\n",
      "Epoch [279/600], Loss: 0.5507\n",
      "Epoch [280/600], Loss: 0.5493\n",
      "Epoch [281/600], Loss: 0.5450\n",
      "Epoch [282/600], Loss: 0.5481\n",
      "Epoch [283/600], Loss: 0.5423\n",
      "Epoch [284/600], Loss: 0.5566\n",
      "Epoch [285/600], Loss: 0.5428\n",
      "Epoch [286/600], Loss: 0.5357\n",
      "Epoch [287/600], Loss: 0.5294\n",
      "Epoch [288/600], Loss: 0.5379\n",
      "Epoch [289/600], Loss: 0.5447\n",
      "Epoch [290/600], Loss: 0.5362\n",
      "Epoch [291/600], Loss: 0.5286\n",
      "Epoch [292/600], Loss: 0.5390\n",
      "Epoch [293/600], Loss: 0.5182\n",
      "Epoch [294/600], Loss: 0.5297\n",
      "Epoch [295/600], Loss: 0.5246\n",
      "Epoch [296/600], Loss: 0.5212\n",
      "Epoch [297/600], Loss: 0.5140\n",
      "Epoch [298/600], Loss: 0.5240\n",
      "Epoch [299/600], Loss: 0.5089\n",
      "Epoch [300/600], Loss: 0.5119\n",
      "Epoch [301/600], Loss: 0.5115\n",
      "Epoch [302/600], Loss: 0.5295\n",
      "Epoch [303/600], Loss: 0.5200\n",
      "Epoch [304/600], Loss: 0.5117\n",
      "Epoch [305/600], Loss: 0.5107\n",
      "Epoch [306/600], Loss: 0.4939\n",
      "Epoch [307/600], Loss: 0.5214\n",
      "Epoch [308/600], Loss: 0.4965\n",
      "Epoch [309/600], Loss: 0.5027\n",
      "Epoch [310/600], Loss: 0.5004\n",
      "Epoch [311/600], Loss: 0.4991\n",
      "Epoch [312/600], Loss: 0.4883\n",
      "Epoch [313/600], Loss: 0.4923\n",
      "Epoch [314/600], Loss: 0.4868\n",
      "Epoch [315/600], Loss: 0.4901\n",
      "Epoch [316/600], Loss: 0.4785\n",
      "Epoch [317/600], Loss: 0.4735\n",
      "Epoch [318/600], Loss: 0.4752\n",
      "Epoch [319/600], Loss: 0.4958\n",
      "Epoch [320/600], Loss: 0.4784\n",
      "Epoch [321/600], Loss: 0.4883\n",
      "Epoch [322/600], Loss: 0.4729\n",
      "Epoch [323/600], Loss: 0.4733\n",
      "Epoch [324/600], Loss: 0.4761\n",
      "Epoch [325/600], Loss: 0.4794\n",
      "Epoch [326/600], Loss: 0.4675\n",
      "Epoch [327/600], Loss: 0.4668\n",
      "Epoch [328/600], Loss: 0.4652\n",
      "Epoch [329/600], Loss: 0.4644\n",
      "Epoch [330/600], Loss: 0.4619\n",
      "Epoch [331/600], Loss: 0.4632\n",
      "Epoch [332/600], Loss: 0.4708\n",
      "Epoch [333/600], Loss: 0.4627\n",
      "Epoch [334/600], Loss: 0.4611\n",
      "Epoch [335/600], Loss: 0.4588\n",
      "Epoch [336/600], Loss: 0.4636\n",
      "Epoch [337/600], Loss: 0.4483\n",
      "Epoch [338/600], Loss: 0.4504\n",
      "Epoch [339/600], Loss: 0.4414\n",
      "Epoch [340/600], Loss: 0.4491\n",
      "Epoch [341/600], Loss: 0.4618\n",
      "Epoch [342/600], Loss: 0.4398\n",
      "Epoch [343/600], Loss: 0.4380\n",
      "Epoch [344/600], Loss: 0.4416\n",
      "Epoch [345/600], Loss: 0.4419\n",
      "Epoch [346/600], Loss: 0.4331\n",
      "Epoch [347/600], Loss: 0.4442\n",
      "Epoch [348/600], Loss: 0.4603\n",
      "Epoch [349/600], Loss: 0.4337\n",
      "Epoch [350/600], Loss: 0.4308\n",
      "Epoch [351/600], Loss: 0.4223\n",
      "Epoch [352/600], Loss: 0.4276\n",
      "Epoch [353/600], Loss: 0.4233\n",
      "Epoch [354/600], Loss: 0.4341\n",
      "Epoch [355/600], Loss: 0.4305\n",
      "Epoch [356/600], Loss: 0.4221\n",
      "Epoch [357/600], Loss: 0.4276\n",
      "Epoch [358/600], Loss: 0.4248\n",
      "Epoch [359/600], Loss: 0.4281\n",
      "Epoch [360/600], Loss: 0.4392\n",
      "Epoch [361/600], Loss: 0.4180\n",
      "Epoch [362/600], Loss: 0.4166\n",
      "Epoch [363/600], Loss: 0.4153\n",
      "Epoch [364/600], Loss: 0.4271\n",
      "Epoch [365/600], Loss: 0.4112\n",
      "Epoch [366/600], Loss: 0.4139\n",
      "Epoch [367/600], Loss: 0.4203\n",
      "Epoch [368/600], Loss: 0.4174\n",
      "Epoch [369/600], Loss: 0.4077\n",
      "Epoch [370/600], Loss: 0.4126\n",
      "Epoch [371/600], Loss: 0.4251\n",
      "Epoch [372/600], Loss: 0.4131\n",
      "Epoch [373/600], Loss: 0.4064\n",
      "Epoch [374/600], Loss: 0.4002\n",
      "Epoch [375/600], Loss: 0.4034\n",
      "Epoch [376/600], Loss: 0.4017\n",
      "Epoch [377/600], Loss: 0.3937\n",
      "Epoch [378/600], Loss: 0.3930\n",
      "Epoch [379/600], Loss: 0.3907\n",
      "Epoch [380/600], Loss: 0.3907\n",
      "Epoch [381/600], Loss: 0.3863\n",
      "Epoch [382/600], Loss: 0.3867\n",
      "Epoch [383/600], Loss: 0.3997\n",
      "Epoch [384/600], Loss: 0.3908\n",
      "Epoch [385/600], Loss: 0.3863\n",
      "Epoch [386/600], Loss: 0.3766\n",
      "Epoch [387/600], Loss: 0.3810\n",
      "Epoch [388/600], Loss: 0.3926\n",
      "Epoch [389/600], Loss: 0.3903\n",
      "Epoch [390/600], Loss: 0.3817\n",
      "Epoch [391/600], Loss: 0.3831\n",
      "Epoch [392/600], Loss: 0.3809\n",
      "Epoch [393/600], Loss: 0.3786\n",
      "Epoch [394/600], Loss: 0.3648\n",
      "Epoch [395/600], Loss: 0.3755\n",
      "Epoch [396/600], Loss: 0.3678\n",
      "Epoch [397/600], Loss: 0.3753\n",
      "Epoch [398/600], Loss: 0.3778\n",
      "Epoch [399/600], Loss: 0.3666\n",
      "Epoch [400/600], Loss: 0.3830\n",
      "Epoch [401/600], Loss: 0.3904\n",
      "Epoch [402/600], Loss: 0.3658\n",
      "Epoch [403/600], Loss: 0.3647\n",
      "Epoch [404/600], Loss: 0.3654\n",
      "Epoch [405/600], Loss: 0.3592\n",
      "Epoch [406/600], Loss: 0.3679\n",
      "Epoch [407/600], Loss: 0.3547\n",
      "Epoch [408/600], Loss: 0.3681\n",
      "Epoch [409/600], Loss: 0.3622\n",
      "Epoch [410/600], Loss: 0.3512\n",
      "Epoch [411/600], Loss: 0.3473\n",
      "Epoch [412/600], Loss: 0.3715\n",
      "Epoch [413/600], Loss: 0.3595\n",
      "Epoch [414/600], Loss: 0.3620\n",
      "Epoch [415/600], Loss: 0.3498\n",
      "Epoch [416/600], Loss: 0.3468\n",
      "Epoch [417/600], Loss: 0.3470\n",
      "Epoch [418/600], Loss: 0.3488\n",
      "Epoch [419/600], Loss: 0.3427\n",
      "Epoch [420/600], Loss: 0.3507\n",
      "Epoch [421/600], Loss: 0.3465\n",
      "Epoch [422/600], Loss: 0.3543\n",
      "Epoch [423/600], Loss: 0.3454\n",
      "Epoch [424/600], Loss: 0.3447\n",
      "Epoch [425/600], Loss: 0.3448\n",
      "Epoch [426/600], Loss: 0.3298\n",
      "Epoch [427/600], Loss: 0.3417\n",
      "Epoch [428/600], Loss: 0.3348\n",
      "Epoch [429/600], Loss: 0.3444\n",
      "Epoch [430/600], Loss: 0.3442\n",
      "Epoch [431/600], Loss: 0.3323\n",
      "Epoch [432/600], Loss: 0.3515\n",
      "Epoch [433/600], Loss: 0.3335\n",
      "Epoch [434/600], Loss: 0.3256\n",
      "Epoch [435/600], Loss: 0.3271\n",
      "Epoch [436/600], Loss: 0.3294\n",
      "Epoch [437/600], Loss: 0.3272\n",
      "Epoch [438/600], Loss: 0.3292\n",
      "Epoch [439/600], Loss: 0.3161\n",
      "Epoch [440/600], Loss: 0.3145\n",
      "Epoch [441/600], Loss: 0.3185\n",
      "Epoch [442/600], Loss: 0.3143\n",
      "Epoch [443/600], Loss: 0.3158\n",
      "Epoch [444/600], Loss: 0.3161\n",
      "Epoch [445/600], Loss: 0.3237\n",
      "Epoch [446/600], Loss: 0.3309\n",
      "Epoch [447/600], Loss: 0.3245\n",
      "Epoch [448/600], Loss: 0.3267\n",
      "Epoch [449/600], Loss: 0.3171\n",
      "Epoch [450/600], Loss: 0.3153\n",
      "Epoch [451/600], Loss: 0.3258\n",
      "Epoch [452/600], Loss: 0.3250\n",
      "Epoch [453/600], Loss: 0.3033\n",
      "Epoch [454/600], Loss: 0.3004\n",
      "Epoch [455/600], Loss: 0.3034\n",
      "Epoch [456/600], Loss: 0.2969\n",
      "Epoch [457/600], Loss: 0.3097\n",
      "Epoch [458/600], Loss: 0.3025\n",
      "Epoch [459/600], Loss: 0.3187\n",
      "Epoch [460/600], Loss: 0.3048\n",
      "Epoch [461/600], Loss: 0.3034\n",
      "Epoch [462/600], Loss: 0.3029\n",
      "Epoch [463/600], Loss: 0.3028\n",
      "Epoch [464/600], Loss: 0.3061\n",
      "Epoch [465/600], Loss: 0.3338\n",
      "Epoch [466/600], Loss: 0.3093\n",
      "Epoch [467/600], Loss: 0.2956\n",
      "Epoch [468/600], Loss: 0.2895\n",
      "Epoch [469/600], Loss: 0.2977\n",
      "Epoch [470/600], Loss: 0.3042\n",
      "Epoch [471/600], Loss: 0.2906\n",
      "Epoch [472/600], Loss: 0.2854\n",
      "Epoch [473/600], Loss: 0.2859\n",
      "Epoch [474/600], Loss: 0.2977\n",
      "Epoch [475/600], Loss: 0.2890\n",
      "Epoch [476/600], Loss: 0.2846\n",
      "Epoch [477/600], Loss: 0.2921\n",
      "Epoch [478/600], Loss: 0.2856\n",
      "Epoch [479/600], Loss: 0.2937\n",
      "Epoch [480/600], Loss: 0.2773\n",
      "Epoch [481/600], Loss: 0.2797\n",
      "Epoch [482/600], Loss: 0.2835\n",
      "Epoch [483/600], Loss: 0.2764\n",
      "Epoch [484/600], Loss: 0.2771\n",
      "Epoch [485/600], Loss: 0.2814\n",
      "Epoch [486/600], Loss: 0.2768\n",
      "Epoch [487/600], Loss: 0.2772\n",
      "Epoch [488/600], Loss: 0.2984\n",
      "Epoch [489/600], Loss: 0.2683\n",
      "Epoch [490/600], Loss: 0.2744\n",
      "Epoch [491/600], Loss: 0.2724\n",
      "Epoch [492/600], Loss: 0.2775\n",
      "Epoch [493/600], Loss: 0.2733\n",
      "Epoch [494/600], Loss: 0.2704\n",
      "Epoch [495/600], Loss: 0.2754\n",
      "Epoch [496/600], Loss: 0.2769\n",
      "Epoch [497/600], Loss: 0.2825\n",
      "Epoch [498/600], Loss: 0.2573\n",
      "Epoch [499/600], Loss: 0.2646\n",
      "Epoch [500/600], Loss: 0.2630\n",
      "Epoch [501/600], Loss: 0.2656\n",
      "Epoch [502/600], Loss: 0.2574\n",
      "Epoch [503/600], Loss: 0.2615\n",
      "Epoch [504/600], Loss: 0.2625\n",
      "Epoch [505/600], Loss: 0.2694\n",
      "Epoch [506/600], Loss: 0.2641\n",
      "Epoch [507/600], Loss: 0.2565\n",
      "Epoch [508/600], Loss: 0.2669\n",
      "Epoch [509/600], Loss: 0.2690\n",
      "Epoch [510/600], Loss: 0.2606\n",
      "Epoch [511/600], Loss: 0.2523\n",
      "Epoch [512/600], Loss: 0.2507\n",
      "Epoch [513/600], Loss: 0.2596\n",
      "Epoch [514/600], Loss: 0.2645\n",
      "Epoch [515/600], Loss: 0.2535\n",
      "Epoch [516/600], Loss: 0.2432\n",
      "Epoch [517/600], Loss: 0.2506\n",
      "Epoch [518/600], Loss: 0.2469\n",
      "Epoch [519/600], Loss: 0.2507\n",
      "Epoch [520/600], Loss: 0.2499\n",
      "Epoch [521/600], Loss: 0.2447\n",
      "Epoch [522/600], Loss: 0.2441\n",
      "Epoch [523/600], Loss: 0.2437\n",
      "Epoch [524/600], Loss: 0.2419\n",
      "Epoch [525/600], Loss: 0.2426\n",
      "Epoch [526/600], Loss: 0.2519\n",
      "Epoch [527/600], Loss: 0.2536\n",
      "Epoch [528/600], Loss: 0.2412\n",
      "Epoch [529/600], Loss: 0.2374\n",
      "Epoch [530/600], Loss: 0.2479\n",
      "Epoch [531/600], Loss: 0.2446\n",
      "Epoch [532/600], Loss: 0.2517\n",
      "Epoch [533/600], Loss: 0.2437\n",
      "Epoch [534/600], Loss: 0.2342\n",
      "Epoch [535/600], Loss: 0.2341\n",
      "Epoch [536/600], Loss: 0.2360\n",
      "Epoch [537/600], Loss: 0.2493\n",
      "Epoch [538/600], Loss: 0.2298\n",
      "Epoch [539/600], Loss: 0.2347\n",
      "Epoch [540/600], Loss: 0.2282\n",
      "Epoch [541/600], Loss: 0.2325\n",
      "Epoch [542/600], Loss: 0.2295\n",
      "Epoch [543/600], Loss: 0.2297\n",
      "Epoch [544/600], Loss: 0.2256\n",
      "Epoch [545/600], Loss: 0.2254\n",
      "Epoch [546/600], Loss: 0.2296\n",
      "Epoch [547/600], Loss: 0.2278\n",
      "Epoch [548/600], Loss: 0.2294\n",
      "Epoch [549/600], Loss: 0.2332\n",
      "Epoch [550/600], Loss: 0.2197\n",
      "Epoch [551/600], Loss: 0.2252\n",
      "Epoch [552/600], Loss: 0.2332\n",
      "Epoch [553/600], Loss: 0.2331\n",
      "Epoch [554/600], Loss: 0.2190\n",
      "Epoch [555/600], Loss: 0.2230\n",
      "Epoch [556/600], Loss: 0.2320\n",
      "Epoch [557/600], Loss: 0.2187\n",
      "Epoch [558/600], Loss: 0.2177\n",
      "Epoch [559/600], Loss: 0.2131\n",
      "Epoch [560/600], Loss: 0.2178\n",
      "Epoch [561/600], Loss: 0.2190\n",
      "Epoch [562/600], Loss: 0.2127\n",
      "Epoch [563/600], Loss: 0.2086\n",
      "Epoch [564/600], Loss: 0.2161\n",
      "Epoch [565/600], Loss: 0.2100\n",
      "Epoch [566/600], Loss: 0.2133\n",
      "Epoch [567/600], Loss: 0.2242\n",
      "Epoch [568/600], Loss: 0.2033\n",
      "Epoch [569/600], Loss: 0.2058\n",
      "Epoch [570/600], Loss: 0.2098\n",
      "Epoch [571/600], Loss: 0.2098\n",
      "Epoch [572/600], Loss: 0.2121\n",
      "Epoch [573/600], Loss: 0.2125\n",
      "Epoch [574/600], Loss: 0.2074\n",
      "Epoch [575/600], Loss: 0.2115\n",
      "Epoch [576/600], Loss: 0.2114\n",
      "Epoch [577/600], Loss: 0.2177\n",
      "Epoch [578/600], Loss: 0.1960\n",
      "Epoch [579/600], Loss: 0.2053\n",
      "Epoch [580/600], Loss: 0.1995\n",
      "Epoch [581/600], Loss: 0.2052\n",
      "Epoch [582/600], Loss: 0.2049\n",
      "Epoch [583/600], Loss: 0.2084\n",
      "Epoch [584/600], Loss: 0.2066\n",
      "Epoch [585/600], Loss: 0.2077\n",
      "Epoch [586/600], Loss: 0.2076\n",
      "Epoch [587/600], Loss: 0.2076\n",
      "Epoch [588/600], Loss: 0.1980\n",
      "Epoch [589/600], Loss: 0.1917\n",
      "Epoch [590/600], Loss: 0.1984\n",
      "Epoch [591/600], Loss: 0.1975\n",
      "Epoch [592/600], Loss: 0.1981\n",
      "Epoch [593/600], Loss: 0.1947\n",
      "Epoch [594/600], Loss: 0.1922\n",
      "Epoch [595/600], Loss: 0.1908\n",
      "Epoch [596/600], Loss: 0.1924\n",
      "Epoch [597/600], Loss: 0.1897\n",
      "Epoch [598/600], Loss: 0.2044\n",
      "Epoch [599/600], Loss: 0.1889\n",
      "Epoch [600/600], Loss: 0.1845\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = create_net()\n",
    "print(f'created network for labelling {len(labels)} classes')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 600\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 70])\n",
      "torch.Size([32, 70])\n",
      "torch.Size([32, 70])\n",
      "torch.Size([32, 70])\n",
      "torch.Size([32, 70])\n",
      "torch.Size([32, 70])\n",
      "torch.Size([18, 70])\n",
      "\n",
      "Accuracy on test set: 0.6857\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nAccuracy on test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Save the model to a file\n",
    "# Save the model to a file with a timestamp-based name\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER):\n",
    "    os.makedirs(MODEL_SAVE_FOLDER)\n",
    "filename = MODEL_SAVE_FOLDER+'/'+f\"model_{timestamp}.pth\"\n",
    "torch.save(model.state_dict(), filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
